# Directory to save trained models
MODEL_DIRECTORY=app/backend/trained_model
# Name of the model to use or train
# Suggested values: 
# 'bert-base-uncased': Well-suited for NER or QA tasks, understands English context well.
# 'gpt2': Known for generating human-like text, suitable for text generation tasks.
# 'roberta-base': Optimized BERT version excelling in deep context understanding.
# 'distilbert-base-uncased': A distilled BERT version, balances speed and accuracy.
# 't5-small': Offers versatility for encoding and decoding tasks, great for translation, summarization, QA, and more.
MODEL_NAME=gpt2  

# Model type, 'auto' detects based on name
# Suggested values: 
# 'auto': Automatically determines model architecture from name, simplifying model selection.
# You can also specify explicitly ('bert', 'gpt2', 'roberta', 'distilbert', 't5') for precise control.
MODEL_TYPE=auto  

# Training configuration settings
# Number of epochs to train the model
# Suggested range: 1-100
NUM_TRAIN_EPOCHS=20
# Batch size per training device (GPU/CPU)
# Suggested range: 8-64
PER_DEVICE_TRAIN_BATCH_SIZE=16
# Number of steps before gradient update
# Suggested values: 1-10 for larger batch training
GRADIENT_ACCUMULATION_STEPS=1
# Use mixed precision training (FP16) for speed
# true for speed, false for precision
FP16=true
# Learning rate for optimizer
# Common range: 1e-5 to 5e-5
LEARNING_RATE=0.00003
# Number of steps for learning rate warmup
# Adjust based on total steps for smoother learning rate increase
WARMUP_STEPS=1000
# Steps between saving the model
# Adjust based on preference and training length
SAVE_STEPS=10000
# Limit on the number of models to save
# Helps manage disk space
SAVE_TOTAL_LIMIT=2
# Strategy for running evaluations ('steps' or 'epoch')
# 'steps' for regular intervals, 'epoch' for end of each epoch
EVALUATION_STRATEGY=steps
# Steps between evaluations
# Adjust based on preference and training speed
EVAL_STEPS=1000
# Use early stopping to prevent overfitting
# Set to true to enable
USE_EARLY_STOPPING=false
# Patience for early stopping
# Number of evaluations with no improvement
EARLY_STOPPING_PATIENCE=3
# Use a learning rate scheduler
# Recommended for better results
USE_LR_SCHEDULER=true
# Type of learning rate scheduler
# 'linear' or other supported types
LR_SCHEDULER_TYPE=linear

# Logging and runtime settings
# Directory for training logs
LOGGING_DIR=logs
# Steps between logging metrics
# Adjust for more or less frequent logging
LOGGING_STEPS=500
# Whether to perform training
DO_TRAIN=true
# Whether to perform evaluation
DO_EVAL=true
# Whether to make predictions
DO_PREDICT=false
# Load the best model at the end based on metric
LOAD_BEST_MODEL_AT_END=true
# Metric to determine the best model
# Use 'accuracy' or other metrics for classification tasks
METRIC_FOR_BEST_MODEL=loss
# Higher metric values indicate better models
# Set to true for metrics where higher is better
GREATER_IS_BETTER=false

# Generation settings (for models that generate text)
# Increment to max length for generation
# Adjust based on desired output length
MAX_LENGTH_INCREMENT=500
# Whether to sample during generation
# Set to false for deterministic output
DO_SAMPLE=true
# Top-K filtering in generation
# Higher values increase diversity
TOP_K=50
# Size of no repeat n-gram
# Prevents repetition, increase for more variety
NO_REPEAT_NGRAM_SIZE=2
# Temperature for generation
# Higher for more creativity, lower for more conservative
TEMPERATURE=0.8
# Top-p (nucleus) filtering
# Higher values increase diversity
TOP_P=0.92

# Path to file for processing or prediction
FILE_PATH=app/backend/input/sample.txt
